---
title: What’s different about causal inference?
author: Alexander D'Amour
date: '2018-05-26'
slug: what-s-different-about-causal-inference
categories:
  - Causal Inference
tags:
  - Causal Inference
  - Identification
---



<div class="note">
<p>This post reprises some content from talks that I gave this past academic year.</p>
</div>
<p>Causal inference is getting wider attention these days, especially after some incendiary remarks by Judea Pearl referring to the deep learning revolution in Maching Learning as <a href="https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/">“just curve fitting”</a>. In repsonse, Ferenc Huszár wrote an <a href="http://www.inference.vc/untitled/">excellent blog post</a> explaining some of the concepts and formalism between causal inference using a more constructive tone. Here, I’d like to offer some of my own thoughts about the core ideas that distinguish causal inference from “vanilla” machine learning, trying to keep the formalism to a minimum. I’d like to give readers a sense of the logical operations that are involved in doing causal inference, and use this to motivate the ideas that causal formalism (e.g., causal graphs, counterfactual notation) is designed to represent.</p>
<div id="causal-inference-as-two-stage-process" class="section level2">
<h2>Causal Inference as Two-Stage Process</h2>
<p>Causal inference is fundamentally a two-stage process. I think of these two stages as <strong>modeling observed data</strong> and <strong>reverse-engineering</strong>. The first stage is the same problem that we try to solve in “vanilla” machine learning or statistical estimation: we try to model the distribution of the observed data as well as we can. This is the stage that Pearl refers to as “just curve fitting”. At this stage, all we care about is being able to predict observed data well.</p>
<p>The second stage is trickier: we try to map summaries of the observed population to causal mechanisms that generated the data. This is an inverse problem, and is generally ill-posed; there are usually many mechansims that could induce the same distribution of observed data. To tackle this reverse-engineering problem, we need to make additional assumptions (beyond what we need to model the observed data) about how the observed data distribution is related to causal mechanisms. In causal jargon, we call this second stage <strong>“identification”</strong>.</p>
<p>Here’s an analogy I like for identification. For most of my life, Antoine de Saint-Exupéry’s <a href="https://en.wikipedia.org/wiki/The_Little_Prince"><em>The Little Prince</em></a> has been one of my favorite books. In the opening of the book, the narrator tells a story from his childhood, when he was reading about boa constrictors swallowing animals whole. He creates his own drawing of a boa that has swallowed an elephant. He shows the drawing to the “grown-ups” and asks them if it scares them. They ask him why they should be scared of a hat.</p>
<img src="/img/hat_cropped.jpg" alt="Snake or hat?" />
<div class="caption">
<p><strong>Fig 1:</strong> First attempt at drawing a snake eating an elephant.</p>
</div>
In response, the narrator creates a second drawing that shows a cross-section of the snake with the elephant inside, and asks the grown-ups again whether they are scared. They tell him to go play outside. <img src="/img/boa_cropped.jpg" alt="Grown-ups require the second drawing to understand the artist’s intent." />
<div class="caption">
<p><strong>Fig 2:</strong> Grown-ups require the second drawing to understand the artist’s intent.</p>
</div>
<p>In this analogy, “the elephant inside the snake” (Fig 2) is the causal mechanism that we’d like to characterize, and the original “hat” drawing (Fig 1) is the observed data distribution. When we do causal inference in practice, we don’t get to see Fig 1 directly. We start with a noisy version.</p>
<img src="/img/hat_samp.jpg" alt="Noisy version of the drawing" />
<div class="caption">
<p><strong>Fig 3:</strong> Noisy version of the drawing.</p>
</div>
<p>From this noisy image, we reconstruct the original “hat” drawing (observed data modeling), then attempt to back out the mechanisms that gave rise to the hat (reverse-engineering). The logical flow of causal inference looks like Fig 4.</p>
<img src="/img/two-stage-causal.png" alt="Schematic showing two-stage structure of causal inference" />
<div class="caption">
<p><strong>Fig 4:</strong> Schematic showing the two-stage structure of causal inference.</p>
</div>
<p>The key point here is that from the observed data (Fig 3), the best we can do by data analysis is to recover the “hat” drawing (Fig 1). The rest of the process involves reasoning about how the “hat” drawing is related to the “elephant in snake” drawing. Sticking with the analogy, without identification, the “hat” drawing is inherently ambiguous, and provides equivalent support to a range of different causal interpretations.</p>
<img src="/img/no-identification.png" alt="Equivalent causal interpretations of the hat drawing (Fig 1)." />
<div class="caption">
<p><strong>Fig 5</strong>: Equivalent causal interpretations of the “hat” drawing (Fig 1).<br/> Credit for middle image: <a href="https://www.boredpanda.com/the-ultimate-ziggy-stardust-the-little-prince-mash-up/">Vera Bucsu</a>.</p>
</div>
<p>Antoine de Saint-Exupéry uses this story as a parable demonstrating one of the central themes of <em>The Little Prince</em>:</p>
<blockquote>
<p>The essential things in life are seen not with the eyes, but with the heart.</p>
</blockquote>
<p>Stretching the analogy, I like to say that <strong>identification is the heart of causal inference</strong>. At the very least, it’s a distinct logical operation that separates causal inference tasks from more standard predictive tasks.</p>
</div>
<div id="causal-formalism" class="section level2">
<h2>Causal Formalism</h2>
<p>Causal inference is often associated with exotic notation and diagrams. This notation is designed to represent the causal mechanisms of interest (“the elephant in the snake”) and to specify conditions for identification. There is some disagreement about what th e</p>
<p>Importantly, identifying assumptions are often unverifiable <em>unless we control the data-generating process</em>. This is why experimentation holds such an esteemed place in causal inference.</p>
</div>
<div id="to-be-incorporated" class="section level2">
<h2>To Be Incorporated</h2>
<p>There are three technical ideas I want to get across about the nature of causal inference:</p>
<ul>
<li>The difference between <strong>causal effects</strong> and <strong>association</strong>.</li>
<li>That causal inference requires <strong>transfer learning</strong> (a.k.a., domain adaptation).</li>
<li>That causal formalism is a way to articulate <strong>assumptions</strong> that justify interpreting (i.e., transferring) observed associations as causal effects.</li>
</ul>
<p>causal inference depends much more on <strong>data collection/generation</strong> than it does on <strong>data analysis</strong>. The assumptions that one makes in causal inference are usually not related to, say, model capacity, but are instead direct assumptions about the data generating process itself. Often the causal relationship we want to study <strong>cannot be characterized by the data we have collected</strong>, so that even if you could model the observed data with zero bias and zero variance, you could not justify coming to a single causal conclusion. If we want to be certain that we can recover the desired causal relationships, we need to control the data-generating process. This is why experimentation holds such an esteemed place in causal inference.</p>
</div>
